runner:
  total_steps: 14400
  gradient_clipping: 1
  gradient_accumulate_steps: 8

  log_step: 500
  eval_step: 100
  save_step: 100
  max_keep: 1
  eval_dataloaders:
    - dev
    - test

optimizer:
  name: TorchOptim
  torch_optim_name: Adam
  lr: 1.0e-5

#scheduler:
#  name: linear_schedule_with_warmup
#  num_warmup_steps: 500
  
downstream_expert:
  listrc:
    audiotype: phrase   # audiotype='phrase','aiu','a_n': denpend on the list (data/lst/train_'audiotype'_'gender'_meta_data_fold*.json)
    gender: both        # gender='both','male','female': depend on the list (data/lst/train_'audiotype'_'gender'_meta_data_fold*.json)
    traindata: train_phrase_both_meta_data_AVFAD
    testdata: test_phrase_both_meta_data_SVD  #calibration/SVD40min/calibration_2minutes  #test_phrase_both_meta_data_SVD
    augment_type: none 
    mixup_alpha: 1 
    mixup_beta: 1  
    batch_augment_n: 1 

  datarc:
    root: downstream/voicedisorder/
    test_fold: fold1
    pre_load: True
    train_batch_size: 16
    eval_batch_size: 16
    num_workers: 4
    valid_ratio: 0.2
  
  visualrc:
    roc: 1            # roc='0','1': compute roc curve at each eval_step
    embeddings: 0     # embeddings: '0','1': write embeddings at each eval step

  modelrc:
    projector_dim: 512
    select: Transformer # Classifier selection: 'Transformer','DeepModel','UtteranceLevel'
    
  lossrc:
    loss_type: CrossEntropyLoss   # ECELoss, LogitNormLoss, AUCLoss (delta_aucloss: ), CrossEntropyLoss, LabelSmoothingCrossEntropyLoss (epsilon: 0.1), BrierLoss
    epsilon: 0.1

 # Uncomment only if select=UtteranceLevel
  #UtteranceLevel:
  #    pooling: AttentivePooling    # options in downstream/model.py:'MeanPooling','AttentivePooling' 

  # Uncomment only if select=DeepModel
  #DeepModel:
  #    model_type: CNNSelfAttention   # options in downstream/pathologies/model.py: 'CNNSelfAttention','FCN'
  #    hidden_dim: 80
  #    kernel_size: 5
  #    padding: 2
  #    pooling: 5
  #    dropout: 0.4 
